\chapter{Sorting}

\section{Introduction}
Why sorting is important

Give an example of sorting
\begin{description}
\item[Stable] When sorting some kinds of data, only part of the data is examined when determining the sort order. For example a deck of card can be orderd by means of the rank ignoring the seed yelding to multiple different valid sorting. A stable sorting algorithm ensure that the order of equal element will be preserved. 
\item[Adaptive]	A sorting algorithm falls into the adaptive sort family if it takes advantage of existing order in its input.  It benefits from the presortedness in the input sequence (or limited disorder) to sort faster. Time complexity is function of input size and entropy.
\item [Online] Online; i.e., can sort a list as it receives it
\end{description}

\section{Bubble-Sort}
Bubble sort, sometimes referred to as sinking sort, is a simple sorting algorithm that repeatedly steps through the list to be sorted, compares each pair of adjacent items and swaps them if they are in the wrong order. The pass through the list is repeated until no swaps are needed, which indicates that the list is sorted. The algorithm, which is a comparison sort, is named for the way smaller elements "bubble" to the top of the list. Although the algorithm is simple, it is too slow and impractical for most problems even when compared to insertion sort (see \ref{sec:insertionsort} at page \pageref{sec:insertionsort}). \textbf{Bubble sort is stable and adaptive}.

The distance and direction that elements must move during the sort determine bubble sort's performance because elements move in different directions at different speeds. An element that must move toward the end of the list can move quickly because it can take part in successive swaps. For example, the largest element in the list will win every swap, so it moves to its sorted position on the first pass even if it starts near the beginning. On the other hand, an element that must move toward the beginning of the list cannot move faster than one step per pass, so elements move toward the beginning very slowly. If the smallest element is at the end of the list, it will take $n-1$ passes to move it to the beginning. This has led to these types of elements being named rabbits and turtles, respectively, after the characters in Aesop's fable of The Tortoise and the Hare.

\begin{lstlisting}[language=C++, caption="Bubble-sort implementation in C++14"]
// CMP_FN has type: D -> D -> bool
template <typename Iterator, typename CMP_FN>
void bubblesort(Iterator s, Iterator e, CMP_FN cmp) {
  auto it1 = s;
  for (; it1 != e; it1++) {
    auto it2 = s;
    for (; it2 != it1; it2++)
      if (cmp(*it1, *it2)) 
      		std::swap(*it1, *it2);
  }
}
\end{lstlisting}

\subsection{Complexity Analysis}
Complexity is $\mathcal{O}(n^2)$ in worst and averge case. Being adaptive it takes advantage of already sorted (or nearly sorted) input. In this case the complexity is $\mathcal{O}(2n)$.
Bubble sort is asymptotically equivalent in running time to insertion sort in the worst case, but the two algorithms differ greatly in the number of swaps necessary. Experimental results such as those of Astrachan have also shown that insertion sort performs considerably better even on random lists. For these reasons many modern algorithm textbooks avoid using the bubble sort algorithm in favor of insertion sort.
Bubble sort also interacts poorly with modern CPU hardware. It requires at least twice as many writes as insertion sort, twice as many cache misses, and asymptotically more branch mispredictions. Experiments by Astrachan sorting strings in Java show bubble sort to be roughly one-fifth as fast as an insertion sort and $70\%$ as fast as a selection sort.
The only significant advantage that bubble sort has over most other implementations, even quicksort, but not insertion sort, is that the ability to detect that the list is sorted efficiently is built into the algorithm. When the list is already sorted (best-case), the complexity of bubble sort is only $\mathcal{O}(2n)$

\section{Insertion-Sort}
\label{sec:insertionsort}
Insertion sort is a simple sorting algorithm that builds the final sorted array (or list) one item at a time. It is much less efficient on large lists than more advanced algorithms such as quicksort, heapsort, or merge sort. However, insertion sort provides several advantages.
It is more efficient in practice than most other simple quadratic (i.e., $\mathcal{O}(n^2)$) algorithms such as selection sort or bubble sort. It is stable and adaptive.Infact when the input is nearly sorted its complexity becomes $\mathcal{O}(nk)$ (when each element is no more than k places away from its sorted position). Can also be easily implemented in-place (using $\mathcal{O}(1)$ additional memory).

It works keeping the first part of the data sorted and iterating on the rest of the data. For each element not already sorted the method finds its sorted position in the first part (the already sorted) of the data.

Sorting is typically done in-place, by iterating up the array, growing the sorted list behind it. At each array-position, it checks the value there against the largest value in the sorted list (which happens to be next to it, in the previous array-position checked). If larger, it leaves the element in place and moves to the next. If smaller, it finds the correct position within the sorted list, shifts all the larger values up to make a space, and inserts into that correct position.
The resulting array after k iterations has the property where the first k + 1 entries are sorted ("+1" because the first entry is skipped). In each iteration the first remaining entry of the input is removed, and inserted into the result at the correct position, thus extending the result:

\begin{figure}
		\includegraphics[width=8cm]{Insertionsort-before.png}
\end{figure}

\begin{figure}
		\includegraphics[width=8cm]{Insertionsort-after.png}
	\end{figure}


\begin{lstlisting}[language=c++, caption="Bubble-sort implementation in C++14"]
// CMP_FN has type: D -> D -> bool
template < typename Container, typename CMP_FN>
void insertionsort(Container& v, const int s,const int e, CMP_FN cmp) {
    for(int i=s+1 ; i<e; i++){
        int el = v[i];
        int j = i-1;
        while(cmp(el,v[j]) && j>=s){
            v[j+1] = v[j];
            j--;
        }
        v[j+1]=el;
    }
}
\end{lstlisting}

Suppose we have an array A, and that the subarray from index 0 through index $k$ is already sorted, and we want to insert the element currently in index $k+1$ into this sorted subarray, so that the subarray from index 0 through index $k+1$ is sorted. 
To insert the element in position $k+1$ into the subarray to its left, we repeatedly compare it with elements to its left , going right to left. Let's call the element in position $k+1$ the key. Each time we find that the key is less than an element to its left, we slide that element one position to the right, since we know that the key will have to go to that element's left. We'll need to do two things to make this idea work: we need to have a slide operation that slides an element one position to the right, and we need to save the value of the key in a separate place (so that it doesn't get overridden by the element to its immediate left). 

Here an Haskell implementation of \textit{insertion sort}
\begin{lstlisting}[language=Haskell,caption="Haskell Insertion sort "]
 insert :: Ord a => a -> [a] -> [a]
 insert item []  = [item]
 insert item (h:t) | item <= h = item:h:t
                   | otherwise = h:(insert item t)

 insertsort :: Ord a => [a] -> [a]
 insertsort []    = []   
 insertsort (h:t) = insert h (insertsort t)
\end{lstlisting}

\subsection{Complexity Analysis}
Time complexity is  $\mathcal{O}(n^2)$ while spatial complexity is  $\mathcal{O}(1)$. It is adaptive, and when each element does not move more than $k$ position from the original position to the sorted location its complexity is linear:  $\mathcal{O}(nk)$.   As for bubble-sort its best case is linear, but it performs much less operations mainly because bubble-sort is a \textit{exchange} algorithm while insertion sort is a \textit{shifting} one. Exchange requires a third of the operation of \textit{shifting}.

\section{Selection-Sort}
Selection sorting is conceptually the most simplest sorting algorithm. This algorithm first finds the smallest element in the array and exchanges it with the element in the first position, then find the second smallest element and exchange it with the element in the second position, and continues in this way until the entire array is sorted. 

The algorithm divides the input list into two parts: the sublist of items already sorted, which is built up from left to right at the front (left) of the list, and the sublist of items remaining to be sorted that occupy the rest of the list. Initially, the sorted sublist is empty and the unsorted sublist is the entire input list. The algorithm proceeds by finding the smallest (or largest, depending on sorting order) element in the unsorted sublist, exchanging (swapping) it with the leftmost unsorted element (putting it in sorted order), and moving the sublist boundaries one element to the right.

\begin{lstlisting}[language=c++, caption="Selection Sort implementation in C++14"]
// CMP_FN has type: D -> D -> bool
template < typename Container, typename CMP_FN>
void selectionsort(Container& v, const int s,const int e, CMP_FN cmp) {
    for(int i=s ; i<=e ;i++){
            int min = v[i];
            int idxmin = i;
            for(int j=i+1 ; j<=e ;j++)
                if(v[j] < min){
                    min = v[j];
                    idxmin = j;
                }
            swap(v[i],v[idxmin]);
    }
}
\end{lstlisting}



\begin{verbatim}
64 25 12 22 11 // this is the initial, starting state of the array

11 25 12 22 64 // sorted sublist = {11}

11 12 25 22 64 // sorted sublist = {11, 12}

11 12 22 25 64 // sorted sublist = {11, 12, 22}

11 12 22 25 64 // sorted sublist = {11, 12, 22, 25}

11 12 22 25 64 // sorted sublist = {11, 12, 22, 25, 64}
\end{verbatim}
\subsection{Complexity Analysis}
Time complexity is  $\mathcal{O}(n^2)$ and is generally slower than insertion sort and is not used in production code except when memory is very limited.





\section{Merge-Sort}
In computer science, merge sort (also commonly spelled mergesort) is an efficient, general-purpose, comparison-based sorting algorithm. Most implementations produce a stable sort, which means that the implementation preserves the input order of equal elements in the sorted output. Mergesort is a divide and conquer algorithm that was invented by John von Neumann in 1945.

Conceptually, a merge sort works as follows:
\begin{enumerate}
\item Divide the unsorted list into n sublists, each containing 1 element (a list of 1 element is considered sorted).
\item Repeatedly merge sublists to produce new sorted sublists until there is only 1 sublist remaining. This will be the sorted list.
\end{enumerate} 

Merge is the most important part of the algorithm. Merge algorithms are a family of algorithms that take multiple sorted lists as input and produce a single list as output, containing all the elements of the inputs lists in sorted order. These algorithms are used as subroutines in various sorting algorithms, most famously merge sort.

In particular merging two sorted collections into one can be done in linear time and linear space.When the inputs are linked lists, this algorithm can be implemented to use only a constant amount of working space; the pointers in the lists' nodes can be reused for bookkeeping and for constructing the final merged list.

\begin{lstlisting}[language=c++, caption="Bubble-sort implementation in C++14"]
// CMP_FN has type: D -> D -> bool
//[s1,e1] and [e1+1,e2] are two ordered sequences. This methods reaggarne the whole
// interval [s1,e2] in a sorted sequence
template < typename Container, typename CMP_FN>
void merge(Container& v, Container& scratch,  int s1,  int e1,  int e2,CMP_FN cmp){
    int s2 = e1+1;
    int ins =0;
    int b = s1;
    while(s1 <= e1 && s2<=e2)
        if(cmp(v[s1],v[s2]))
            scratch[b+ins++] = v[s1++];
        else
            scratch[b+ins++] = v[s2++];
    while(s1 <= e1)
        scratch[b+ins++] = v[s1++];
    while(s2 <= e2)
        scratch[b+ins++] = v[s2++];
        
    for(int i=0 ; i < ins ; i++)
        v[b+i] = scratch[b+i];
}
\end{lstlisting}

The following is the main mergesort method. It uses an helper function in order to hide the additional space required by the merge. Note that midpoint is computed using the \texttt{safe\_midpoint()} methods which avoids overflows for large $lo$ and $hi$ at a cost of an additional operation.

\begin{lstlisting}[language=c++, caption="Merge-sort"]
template<typename T>
inline T safe_midpoint(const T lo, const T hi){
    return lo+(hi-lo)/2;
}
template<typename T>
inline T unsafe_midpoint(const T lo, const T hi){
    return (hi+lo)/2;
}
#define TRIGGER_INSERTIONSORT (4)
template < typename Container, typename CMP_FN>
void mergesort_helper(Container& v,Container& scratch, const int s,const int e, CMP_FN cmp) {
    if(e -s <= TRIGGER_INSERTIONSORT)
        DS::insertionsort(v,s,e,cmp);
    if(s < e){
        int midpoint = safe_midpoint(s,e);
        mergesort_helper(v,scratch,s,midpoint,cmp);
        mergesort_helper(v,scratch,midpoint+1,e,cmp);
        merge(v,scratch,s,midpoint,e,cmp);
    }
}
// CMP_FN has type: D -> D -> bool
template < typename Container, typename CMP_FN>
void mergesort(Container& v, const int s,const int e, CMP_FN cmp) {
    Container scratch(v.size());
    mergesort_helper(v,scratch,s,e,cmp);

}
\end{lstlisting}


\textbf{QuickSort is better for contiguous data structures}.
In the merge sort algorithm, this subroutine is typically used to merge two sub-arrays $A[lo..mid]$, $A[mid..hi]$ of a single array $A$. This can be done by copying the sub-arrays into a temporary array, then applying the merge algorithm above.The allocation of a temporary array can be avoided, but at the expense of speed and programming ease. Various in-place merge algorithms have been devised, sometimes sacrificing the linear-time bound to produce an $\mathcal{nlog(n)}(n^2)$ algorithm.

Quick Sort in its general form is an in-place sort (i.e. it does not require any extra storage) whereas merge sort requires $O(N)$ extra storage, where $N$ denote the array size which may be quite expensive. Allocating and de-allocating the extra space used for merge sort increases the running time of the algorithm. Comparing average complexity we find that both type of sorts have $O(NlogN)$ average complexity but the constants differ (and sometimes matter). For arrays, merge sort loses due to the use of extra $O(N)$ storage space.

Most practical implementations of Quick Sort use randomized version. The randomized version has expected time complexity of O(nLogn). The worst case is possible in randomized version also, but worst case doesn’t occur for a particular pattern (like sorted array) and randomized Quick Sort works well in practice.
Quick Sort is also a cache friendly sorting algorithm as it has good locality of reference when used for arrays.
Quick Sort is also tail recursive, therefore tail call optimizations is done.

\textbf{Merge-Sort is better for Linked Data Structure}.
List case is different mainly due to differenc in memory allocation patterns w.t.r. to array.In linked list, we can insert items in the middle in $O(1)$ extra space and time. Therefore merge operation of merge sort can be implemented without extra space for linked lists (i.e. no waste of time during allocation and copy of any extra buffers). 
For contiguous data structure, we can do $O(1)$ time and space random access as elements are continuous in memory. But  Unlike arrays, we can not do random access in linked list \footnote{In linked list to access $i^{th}$ element, we have to travel each and every node from the head to $i^{th}$ node as we don not have continuous block of memory}. Quick Sort requires a lot of this kind of accesses. Therefore, the overhead increases for quick sort. Merge sort accesses data sequentially and the need of random access is low.


\subsubsection{Iterative Merge-Sort}

\subsection{Complexity Analysis}

Merge Sort is a recursive algorithm and time complexity can be expressed as following recurrence relation.
$T(n) = 2T(n/2) + \Theta(n)$
The above recurrence can be solved either using Recurrence Tree method or Master method. It falls in case II of Master Method and solution of the recurrence is $\Theta(nlog(n))$.
Time complexity of Merge Sort is $\Theta(nlog(n))$ in all 3 cases (worst, average and best) as merge sort always divides the array in two halves and take linear time to merge two halves.
Auxiliary Space: $O(n)$
Sorting In Place: No in a typical implementation
Stable: Yes




\section{Quick-Sort}

\begin{lstlisting}[language=c++, caption="QuickSort Partition"]
template<class Container, typename CMP_FN>
inline int selectpivotidx(Container& v, const int s, const int e,CMP_FN cmp){
    return e-1;
}

template<class Container, typename CMP_FN>
 int partition(Container& v, const int s, const int e,CMP_FN cmp){
//Lomuto partition scheme
    int pivotidx = selectpivotidx(v,s,e,cmp);
    swap(v[pivotidx],v[e-1]);//put pivot in the last position
    pivotidx = e-1;
    const auto pivot = v[pivotidx];//last element
    int i = s;
    for(int j=s ; j<e ;j++)
        if(cmp(v[j],pivot) )
            swap(v[i++],v[j]);
    swap(v[i],v[pivotidx]);
    return i;
}
\end{lstlisting}



\begin{lstlisting}[language=c++, caption="QuickSort Recursive"]
#define TRIGGER_INSERTION (7)
// CMP_FN has type: D -> D -> bool
template < typename Container, typename CMP_FN>
inline void quicksort(Container& v, const int s,const int e, CMP_FN cmp) {
    if(e - s <= TRIGGER_INSERTION)
        DS::insertionsort(v,s,e,cmp);
    else{
        const int p = partition(v,s,e,cmp);
        quicksort(v,s,p,cmp);
        quicksort(v,p+1,e,cmp);
    }
}
\end{lstlisting}


\begin{lstlisting}[language=c++, caption="QuickSort Tail Recursive"]
//tail recursive version
// CMP_FN has type: D -> D -> bool
template < typename Container, typename CMP_FN>
inline void quicksort_tr(Container& v,  int s,const int e, CMP_FN cmp) {
    if(e - s <= TRIGGER_INSERTION)
        DS::insertionsort(v,s,e,cmp);
    else
        while(s < e){
        const int p = partition(v,s,e,cmp);
        quicksort(v,s,p,cmp);
        s = p+1;
        }
}
\end{lstlisting}


\subsection{Complexity Analysis}

\section{Heap-Sort}
\subsection{Heap}
Recall that a heap is a data structure that supports the main priority queue operations (\textit{insert} and \textit{extract max}) in $\mathcal{\Theta}(log n)$ time each. It consists of a left-complete binary tree (meaning that all levels of the tree except possibly the bottommost) are full, and the bottommost level is filled from left to right. As a consequence, it follows that the depth of the tree is $\mathcal{\Theta}(log n)$ where n is the number of elements stored in the tree. The keys of the heap are stored in the tree in what is called heap order. This means that for each (\textbf{nonroot}) node its parent’s key is at least as large as its key. From this it follows that the largest key in the heap appears at the root.
\subsection{Heap as Array}
Last time we mentioned that one of the clever aspects of heaps is that they can be stored in
arrays, without the need for using pointers (as would normally be needed for storing binary trees). The
reason for this is the left-complete nature of the tree.
This is done by storing the heap in an array $A[1..n]$. Generally we will not be using all of the array,
since only a portion of the keys may be part of the current heap. For this reason, we maintain a variable
$m \leq n$ which keeps track of the current number of elements that are actually stored actively in the
heap. Thus the heap will consist of the elements stored in elements $A[1..m]$.
We store the heap in the array by simply unraveling it level by level. Because the binary tree is leftcomplete, we know exactly how many elements each level will supply. The root level supplies 1 node,
the next level 2, then 4, then 8, and so on. Only the bottommost level may supply fewer than the
appropriate power of 2, but then we can use the value of m to determine where the last element is. This
is illustrated below. We should emphasize that this only works because the tree is left-complete. This cannot be used for
general trees.

\begin{figure}
\label{fig:heapasarray}
		\includegraphics[width=12cm]{heapasarray.png}
\end{figure}
We claim that to access elements of the heap involves simple arithmetic operations on the array indices.
In particular it is easy to see the following:
\begin{description}
\item[\textit{left(i)}]: $2i$
\item[\textit{right(i)}]: $2i+1$
\item[\textit{parent(i)}]: $\floor{\frac{i}{2}}$
\item[\textit{isRoot(i)}]: $i \neq 0$
\end{description}

So is a heap a binary tree or an array? The answer is that from a conceptual standpoint, it is a binary
tree. However, it is implemented (typically) as an array for space efficiency

\begin{problem}
What are the minimum and maximum numbers of elements in a heap of height h?
\end{problem}

\begin{problem}
Show that an n-element heap has height lg n.
\end{problem}

\begin{problem}

Show that the largest element in a subtree of a heap is at the root of the subtree.
\end{problem}

\begin{problem}

Where in a heap might the smallest element reside?
\end{problem}

\begin{problem}

Is an array that is in reverse sorted order a heap?

\end{problem}

\begin{problem}

Is the sequence 23, 17, 14, 6, 13, 10, 1, 5, 7, 12 a heap?
\end{problem}

\subsection{Maintaining the Heap Property}
There is one principal operation for maintaining the heap property. It is
called \textit{Heapify}. (In other books it is sometimes called sifting down.) The idea is that we are given an element of the heap which we suspect may not be in valid heap order, but we assume that all of other the elements in the subtree rooted at this element are in heap order. In particular this root element may be too small(or too big in a min-heap). To fix this we “shift” or "bubble" it down the tree by swapping it with one of its children. Which child? We should take the larger (or smaller in a min-heap) of the two children to satisfy the heap ordering property. This continues recursively until the element is either larger than both its children or until its falls all the way to the leaf level. Here is the pseudocode. It is given the heap in the array A, and the index $i$ of the suspected element, and $m$ the current active size of the heap. The element $A[max]$ is set to the maximum of $A[i]$ and it two children. If $max \neq i$ then we swap $A[i]$ and $A[max]$ and then recurse on $A[max]$.


\begin{lstlisting}[language=c++, caption="Heapify procedure in heapsort"]
template<class Container,typename CMP_FN>
inline void heapify(Container& h, const int i, const int m, CMP_FN cmp_fn){
    int l,r,max=i;
    l = left_child(i);
    r = right_child(i);
    if(l < m && cmp_fn(h[max],h[l]) )  max = l;
    if(r < m && cmp_fn(h[max],h[r]) )  max = r;
    if(max != i){ //the heap property was violated
        swap(h[i],h[max]);
        heapify(h,max,m,cmp_fn);
    }
}
\end{lstlisting}
This will requires at most as many steps as the height of the tree which is $\mathcal{\Theta}(log n)$.
\subsection{Creating a Heap}
We can use Heapify to build a heap as follows. First we start with a heap in which the elements are not in heap order. They are just in the same order that they were given to us in the array A. We build the heap by starting at the leaf level and then invoke Heapify on each node. (Note: We cannot start at the top of the tree. Why not? Because the precondition which Heapify assumes is that the entire tree rooted at node i is already in heap order, except for i.) Actually, we can be a bit more efficient. Since we know that each leaf is already in heap order, we may as well skip the leaves and start with the first nonleaf node. This will be in position $\floor{\frac{m}{2}}$. (Can you see why?)
Here is the code. Since we will work with the entire array, the parameter m for Heapify, which indicates
the current heap size will be equal to n, the size of array A, in all the calls.

\begin{lstlisting}[language=c++, caption="Create an heap from an unordered collection. cmp\_fn determines if it will be a min or max heap"]
template < typename Container, typename CMP_FN>
void buildheap(Container& c,const int s, const int e, CMP_FN cmp_fn){
    int m = e-s; //e not taken in consideration
    //note that we could start at n/2 since the last n/2 element are leafs
    //and  for those elements the heap property trivially hold
    for(int i= s+(m/2) ; i >= s ; i--)
        heapify(c , i ,m, cmp_fn);
}
\end{lstlisting}
 Since each call to heapify takes $\mathcal{O}(log n)$ time, and we make roughly $\frac{n}{2}$ calls to it, the total running time is $\mathcal{O}(\frac{n}{2} log n)$ = $\mathcal{O}(nlog n)$. Next time we will show that this actually runs faster, and in fact it runs in $\mathcal{\Theta}(n)$ time.
 
\begin{problem}

What is the effect of calling HEAPIFY(A, i) when the element A[i] is larger than its children?
\end{problem}

\begin{problem}

What is the effect of calling HEAPIFY (A, i) for i > heap-size [A]/2?
\end{problem}

\begin{problem}

The code for HEAPIFY is quite efficient in terms of constant factors, except possibly for the recursive call in line 10, which might cause some compilers to produce inefficient code. Write an efficient HEAPIFY that uses an iterative control construct (a loop) instead of recursion.
\end{problem}

\begin{problem}

Show that the worst-case running time of HEAPIFY on a heap of size n is (lg n). (Hint: For a heap with n nodes, give node values that cause HEAPIFY to be called recursively at every node on a path from the root down to a leaf.)
\end{problem}



 
 
 
\subsection{Sorting using a Heap}
The heap gives us easy and fast $\mathcal{O}(log n)$ access to the max/min element of an unordered set.
The algorithm repeatedly extracts the max/min (which will be at first position). This will leave the array with an hole. What we can do to avoid this is to swap this element with last. This can cause heap property to fail, but our heapify procedure can take can of this since the root is the only node that can violate the property and its left and right subtree are correct heaps.
Each extraction cost at most $\mathcal{O}(log n)$. We extract $m-1$ minimum element so the overall complxity is $\mathcal{O}((n-1)log n)$ = $\mathcal{O}(nlog n)$

\begin{lstlisting}[language=c++, caption="Create an heap from an unordered collection. cmp\_fn determines if it will be a min or max heap"]
template < typename Container, typename CMP_FN>
void heapsort(Container& v, const int s,const int e, CMP_FN cmp_fn) {
    int size = e-s;
    buildheap(v,s,e,cmp_fn);
    int m = size;
    while( m > 1){ //last element is in proper order
        swap(v[s],v[-1+s+m--]); //swap(v[s],v[s+m-1]); m--;more readable 
        heapify(v,s,m,cmp_fn); //heap order could not hold at root.
    }
}
\end{lstlisting}

\subsection{Complexity Analysis}
We argued that the basic heap operation of \textit{heapify} runs in $\mathcal{O}(log n)$ time, because the heap has $\mathcal{\Theta}(log n)$ levels, and the element being shifted moves down one level of the tree after a constant amount of work.
Based on this we can see that:
\begin{enumerate}
\item that it takes $\mathcal{O}(nlog n)$ time to build a heap, because we need to apply \texttt{heapify} roughly $\frac{n}{2}$ times (for each of the non-leaf nodes)
\item it takes $\mathcal{O}(nlog n)$ time to extract each of the maximum elements, since we need to extract roughly n elements and each extraction involves a constant amount of work and one Heapify
\end{enumerate}
 Therefore the total running time of \textbf{HeapSort} is $\mathcal{O}(nlog n)$.
Is this tight? That is, is the running time $\mathcal{\Theta}(nlog n)$? The answer is yes. In fact, later we will see that it is not possible to sort faster than $\mathcal{\Omega}(nlog n)$ time, assuming that you use comparisons, which \textbf{HeapSort} does. 

However, it turns out that the first part of the analysis is not tight. In particular, the \textit{BuildHeap} procedure that we presented actually runs in linear time!. Although in the wider context of the \textit{HeapSort} algorithm this is not significant (because the running time is dominated by the $\mathcal{\Theta}(nlog n)$ extraction phase.

\begin{framed}
\textbf{Nonetheless there are situations where you might not need to sort all of the elements. For example, it is common to extract some unknown number of the smallest elements until some criterion (depending
on the particular application) is met ( the first k-smallest for instance). For this reason it is very important to know  that it can be done in linear time.}

Constructing the heap requires to perform a certain amount of work depending on the level of the node. In particular at the last level no work has to be done (nodes  that level do not have any subtree and so they don't need to be \textit{heapified}).
The $\frac{n}{4}$ nodes at the before-last level faces at most one bubble-up operation.
The $\frac{n}{8}$ nodes at the upper level faces at most two bubble-up operations. The pattern is clear and can be summarized using the folliwing:

\[
\sum_{i=2}^{log(n)} \frac{n}{2^{i}}[(i-1)c] = n \sum_{i=2}^{log(n)} \frac{1}{2^{i}}[(i-1)c]
\]
The term inside the summation is bounded by a constant so the overall work is $\mathcal{O}(n)$!


\end{framed}
\begin{figure}
\label{fig:heaplinear}
\caption{"Building an heap in linear time example"}
		\includegraphics[width=12cm]{heaplinear.png}
\end{figure}


\section{Smoot-Sort}

\subsection{Complexity Analysis}


\section{Solving Recurrence relation}

\section{Substitution method}
\label{sec:substitutionmethod}
Despite the name, this is not a method that gives us a solution for the recurrence. It serves only to prove that an initial solution guess is right. It basically use induction to prove that the guess holds and is an actual solution of the recurrence equation.
So let's say we have the following recurrence equation ( we will see later on that we can use the master method for this):
\[T(n) = 2T(\frac{n}{2}) + n\]
Our guess is that $T(n) \leq c(nlog(n))$ (so basically that $T(n) \in O(nlog(n))$), and we will use mathematical induction to prove it.

\begin{itemize}
\item Base Case: $n=0$, $T(n) = n \leq nlog(n)$
\item Assume that it hold for values lower than a certain $n$
\item Inductive Step: 
\[T(n) = 2T(\frac{n}{2}) + n \leq \frac{n}{2} log(\frac{n}{2}) + n \leq nlog(n) - nlog(2) +n \leq nlog(n) - n +n \leq nlog(n)\]
\end{itemize}




\section{Recursion Tree method}
A recursion tree is useful for visualizing what happens when a recurrence is iterated. It diagrams the tree of recursive calls and the amount of work done at each call.
For instance, consider the recurrence
\[T(n) = 2T(\frac{n}{2}) + n^2\]
The recursion tree for this recurrence has the following form(see figure \ref{fig:rectree1})

\begin{figure}
\label{fig:rectree1}
		\includegraphics[width=12cm]{recTree1.png}
\end{figure}
It is easy to compute the amount of work done at each recursion level simply summing up all the nodes at a certain level (see figure \ref{fig:rectree2}).
\begin{figure}
\label{fig:rectree2}
		\includegraphics[width=12cm]{recTree2.png}
\end{figure}

In this case this is a geometric serie which sum limit is bounded by a quadratic function i.e. $O(n^2)$. 

Recursion trees can be useful for gaining intuition about the closed form of a recurrence, but they are not a proof (and in fact it is easy to get the wrong answer with a recursion tree, as is the case with any method that includes ''...'' kinds of reasoning). As we saw last time, a good way of establishing a closed form for a recurrence is to make an educated guess and then prove by induction that your guess is indeed a solution (see section \ref{sec:substitutionmethod} at page \ref{sec:substitutionmethod}). Recurrence trees can be a good method of guessing.


For example consider the following recurrence equation:
\[ T(n) = T(\frac{n}{3}) + T(\frac{2n}{3}) + n\]

Expanding the first levels of the corrensponding recursion tree we obtain the tree in figure \ref{fig:rectree3}.
\begin{figure}
\label{fig:rectree3}
		\includegraphics[width=12cm]{recTree3.png}
\end{figure}

Each level sums up to $n$, the height of this tree is $log_{\frac{3}{2}}(n)$. So out guess is that $T=O(nlogn)$.

Using the substitution method let's check if our guess is right.

\section{Master method}
The master method is a cookbook method for solving recurrences. Although it cannot solve all recurrences, it is nevertheless very handy for dealing with many recurrences seen in practice. It can only cope with recurrences of the following form:

\[T(n) = aT(\frac{n}{b}) +f(n)\]
where a and b are arbitrary constants and f is some function of n. This recurrence would arise in the analysis of a recursive algorithm that for large inputs of size n breaks the input up into a subproblems each of size n/b, recursively solves the subproblems, then recombines the results. \textbf{The work to split the problem into subproblems and recombine the results is $f(n)$}.

We can visualize this as a recurrence tree, where the nodes in the tree have a branching factor of $a$. The root node has work $f(n)$ associated with it, the next level has work $f(\frac{n}{b})$ associated with each of a nodes, the next level has work  $f(\frac{n}{b^2})$ associated with each of $a^2$ nodes, and so on. At the leaves are the base case corresponding to some $1 \leq n < b$. The tree has $log_b(n)$ levels, so the total number of leaves is $a^{log_b(n)} = n^{log_b(a)}$ (see footonate 
\footnote{Since $b^{log_b(x)} = x$ Exponentiation is the inverse operation of logarithm so is like writing $f^{-1}(f(x))$ which is clearly $x$) then:
\[a^{log_b(n)} = b^{log_b(a) \times log_b(n) } = (b^{log_b(n)})^{log_b(a)} = n^{log_b(a)}\]}).

The total number of operation is the sum of the operation performed at each of the $a^{log_b(n)}$ levels.
The number of operation performed at level $i$ is the sum of the time taken by a single node at level $i$ multiplied by the number of nodes at level $i$ plus the constant amount of work done at the leaves.
The number of nodes at level $i$ is $a^{i}$ while the number of operation per node is $f(\frac{n}{b^i})$ .

The total work is then:
\[
(\sum_{i=0}^{log_b(n)} a^i f(\frac{n}{b^i})) + O(n^{log_b(a)}
\]

The asymptotic bound of this sum depends the asymptotic growth of $f(n)$. we can then distinguish three cases:

\begin{enumerate}
\item Case 1: $f(n)$ is $O(n^{log_b(a)-\epsilon})$ i.e. it grows slower than the number of leaves, meaning that all the work is done at leaves. $T(N)$ is then bounded on the number of leaves i.e. $T(N) = \Omega(n^{log_b(a)})$
	
\item Case 2: $f(n)$ is $O(n^{log_b(a)})$ i.e. it grows at the same rate as the number of leaves, meaning that all the work is distributed evenly across all the nodes of all levels of the tree. The tree has $\Omega(log_b(n)$ levels and so $T(N)= \Omega(n^{log_b(a)} log_b(n))$
	
\item Case 3: The easiest one. $f(n)$ is $O(n^{log_b(a) +\epsilon})$ i.e. at limit it grows faster then the number of leaves, meaning that all the work is dominated by the work done at the root of the tree. In this case then  $T(N)= \Omega(f(n))$ ($f$ for large $n$ should also satisfy that $a(f(\frac{n}{b}) \leq cf(n)$ for some constant $c<1$).
	
\end{enumerate} 

\begin{example}
\[
T(n) = 4T(\frac{n}{2}) + n
\]
This recurrence describes a problem for which at each level of recursion $4$ new problems each of size $\frac{n}{2}$ and performing a \textbf{linear} work ($f(n) =n$) at each call.
The number of leaves is then $n^{log_b(a) = n^{log_2(4)} = n^2} $ which grows faster than $f(n)$. \textbf{Case 1} applies giving rise to $T(N) = \Omega(n^2)$.
\end{example}


\begin{example}
\[
T(n) = 4T(\frac{n}{2}) + n^2
\]
This recurrence describes a problem for which at each level of recursion $4$ new problems each of size $\frac{n}{2}$ and performing a \textbf{quadratic} work ($f(n) =n$) at each call.
The number of leaves is again $n^{log_b(a)} = n^{log_2(4)} = n^2 $ which grows as fast as faster $f(n)$. \textbf{Case 2} applies giving rise to $T(N) = \Omega(log_2^{n} n^2)$. Note here that an increase from linear to quadratic in the work perofmed at each call give rise only to a $log$ increase in the overall complexity.

\end{example}

\begin{example}
\[
T(n) = 4T(\frac{n}{2}) + n^3
\]
This recurrence describes a problem for which at each level of recursion $4$ new problems each of size $\frac{n}{2}$ and performing a \textbf{cubic} work ($f(n) =n$) at each call.
The number of leaves is again $n^{log_b(a)} = n^{log_2(4)} = n^2 $ which grows slower than $f(n)$. \textbf{Case 3} applies giving rise to $T(N) = \Omega(n^3)$ ($4(\frac{n}{2})^3 \leq kn^3$ for $k=\frac{1}{2}$). 

\end{example}


\begin{example}

\[T(n) = 2T(\frac{n}{2}) + n\]

This recurrence describes a problem for which at each level of recursion $2$ new problems each of size $\frac{n}{2}$ and performing a \textbf{linear} work ($f(n) =n$) at each call.
The number of leaves is again $n^{log_b(a)} = n^{log_2(2)} = n $ which grows as fast as $f(n)$. \textbf{Case 2} applies giving rise to $T(N) = \Omega(n log(n))$ (as proved by induction in section \ref{sec:substitutionmethod})

\end{example}



\section{Questions \& Problems}
%---Question ------------
\begin{problem}
Which of the following is not a stable sorting algorithm in its typical implementation.
\begin{enumerate}
\item Insertion-sort
\item Merge-sort
\item Quick-sort
\item Bubble-sort
\end{enumerate}

\end{problem}

%---Question------------
\begin{problem}
Consider a situation where swap operation is very costly. Which of the following sorting algorithms should be preferred so that the number of swap operations are minimized in general?
\begin{enumerate}
\item Insertion-sort
\item Merge-sort
\item Quick-sort
\item Bubble-sort
\end{enumerate}

\end{problem}



%---Question------------
\begin{problem}
You have to sort 1 GB of data with only 100 MB of available main memory. Which sorting technique will be most appropriate?
\begin{enumerate}
\item Insertion-sort
\item Merge-sort
\item Quick-sort
\item Bubble-sort
\end{enumerate}

\end{problem}


%---Question------------
\begin{problem}
In a modified merge sort, the input array is splitted at a position one-third of the length(N) of the array. What is the worst case time complexity of this merge sort?
\begin{enumerate}
\item $\mathcal{O}(nlog_3(n))$
\item $\mathcal{O}(nlog_{\frac{2}{3}}(n))$
\item $\mathcal{O}(nlog_{\frac{1}{3}}(n))$
\item $\mathcal{O}(nlog_{\frac{3}{2}}(n))$

\end{enumerate}

\end{problem}


%---Question------------
\begin{problem}
Which sorting algorithm will take least time when all elements of input array are identical? Consider typical implementations of sorting algorithms.
\begin{enumerate}
\item Insertion-sort
\item Merge-sort
\item Quick-sort
\item Bubble-sort
\end{enumerate}

\end{problem}


%---Question------------
\begin{problem}
A list of n string, each of length n, is sorted into lexicographic order using the merge-sort algorithm. The worst case running time of this computation is:
\begin{enumerate}
\item $\mathcal{O}(nlog(n))$
\item $\mathcal{O}(n^2log(n))$
\item $\mathcal{O}(n^2 + log(n))$
\item $\mathcal{O}(n^2)$
\end{enumerate}

\end{problem}


%---Question1------------
\begin{problem}
Which of the following sorting algorithms has the lowest worst-case complexity?
\begin{enumerate}
\item Insertion-sort
\item Merge-sort
\item Quick-sort
\item Bubble-sort
\end{enumerate}

\end{problem}



%---Question1------------
\begin{problem}
Which of the following is true about merge sort?
\begin{enumerate}
\item Merge Sort works better than quick sort if data is accessed from slow sequential memory.
\item Merge Sort is stable sort by nature
\item Merge sort outperforms heap sort in most of the practical situations.
\item All the above.
\end{enumerate}

\end{problem}


%---Question1------------
\begin{problem}
Assume that a mergesort algorithm in the worst case takes 30 seconds for an input of size 64. Which of the following most closely approximates the maximum input size of a problem that can be solved in 6 minutes?
\begin{enumerate}
\item $2^8$
\item $2^9$
\item $2^{10}$
\item $2^{11}$
\end{enumerate}

\end{problem}



%---Problem------------
\begin{problem}
\textit{Given an array, write a program that prints 1 if array is sorted in non-decreasing order, else prints 0.}

\textbf{Input:}
The first line of input contains an integer $T$ denoting the number of test cases.
The first line of each test case contains $N$, which is the size of array (on the following line).
The second line of each test case contains $N$ input $C[i]$.

\textbf{Output:}
Print 1 if array is sorted, else print 0.

\textbf{Constraints:}
\begin{multline}\\
1 \leq T \leq 100\\
1 \leq N \leq 500\\
1 \leq C[i] \leq 1200\\
\end{multline}
\end{problem}

\begin{solution}
\begin{lstlisting}[language=C++, caption="C++ Solution"]
	int main(){
		return 0;	
	}
\end{lstlisting}

\end{solution}



%---Problem------------
\begin{problem}
\textit{Given a binary array, sort it in non-decreasing order.}

\textbf{Input:}
First line contains an integer denoting the test cases $T$.  Every test case contains two lines, first line is size $N$ and second line is space separated elements of array

\textbf{Output:}
Space separated elements of sorted arrays.  There should be a new line between output of every test case.

\textbf{Constraints:}
\begin{multline}\\
10 \leq T \leq 100\\
1 \leq N \leq 10000\\
\end{multline}

\textbf{Example:}
\begin{verbatim}
Input:
2
5
1 0 1 1 0
10
1 0 1 1 1 1 1 0 0 0

Output:
0 0 1 1 1
0 0 0 0 1 1 1 1 1 1 

\end{verbatim}

\end{problem}

\begin{solution}
\begin{lstlisting}[language=C++, caption="C++ Solution"]
	int main(){
		return 0;	
	}
\end{lstlisting}

\end{solution}



%---Problem------------
\begin{problem}
\textit{Given an array, print k largest elements from the array.}

\textbf{Input:}
The first line of input contains an integer $T$ denoting the number of test cases.
The first line of each test case is $N$ and $K$, where $N$ is the size of array and $K$ is the largest elements to be returned.
The second line of each test case contains $N$ input $C[i]$.

\textbf{Output:}
Print the $K$ largest elements in \textbf{descending order}.

\textbf{Constraints:}
\begin{multline}\\
10 \leq T \leq 50\\
1 \leq N \leq 100\\
K \leq N \\
1 \leq C[i] \leq 1000\\
\end{multline}

\textbf{Example:}
\begin{verbatim}
Input:
2
5 2
12 5 787 1 23
7 3
1 23 12 9 30 2 50

Output:
787 23
50 30 23

\end{verbatim}

\end{problem}

\begin{solution}
\begin{lstlisting}[language=C++, caption="C++ Solution"]
	int main(){
		return 0;	
	}
\end{lstlisting}

\end{solution}



%---Problem------------
\begin{problem}
Given an array with n distinct elements, convert the given array to a reduced form where all elements are in range from $[0 ... n-1]$. The order of elements is same, i.e., 0 is placed in place of smallest element, 1 is placed for second smallest element,\ldots $n-1$ is placed as last element.

\textbf{Input:}
The first line of input contains an integer $T$ denoting the number of test cases.
The first line of each test case is $N$, where $N$ is the size of array.
The second line of each test case contains $N$ input $A[i]$.

\textbf{Output:}
Print the reduced form of the array.

\textbf{Constraints:}
\begin{multline}\\
1 \leq T \leq 100\\
1 \leq N \leq 200\\
1 \leq A[i] \leq 500\\
\end{multline}

\textbf{Example:}
\begin{verbatim}
Input:
2
3
10 40 20
5
5 10 40 30 20

Output:
0 2 1
0 1 4 3 2

\end{verbatim}

\end{problem}


%---Problem------------
\begin{problem}
Given two array $A1$ and $A2$, sort $A1$ in such a way that the relative order among the elements will be same as those are in $A2$. For the elements not present in $A2$. Append them at last in sorted order.

\textbf{Input:}
The first line of input contains an integer $T$ denoting the number of test cases.
The first line of each test case is $M$ and $N,M$ are the number of elements in $A1$ and $A2$ respectively.
The second line of each test case contains $M$ elements.
The third line of each test case contains $N$ elements.
\textbf{Output:}
Print the sorted array according order defined by another array.

\textbf{Constraints:}
\begin{multline}\\
1 \leq T \leq 50\\
1 \leq N \leq M \leq 10\\
1 \leq A1[i],A2[i] \leq 500\\
\end{multline}

\textbf{Example:}
\begin{verbatim}
Input:
2
3
10 40 20
5
5 10 40 30 20

Output:
0 2 1
0 1 4 3 2

\end{verbatim}

\end{problem}


\begin{solution}
\begin{lstlisting}[language=C++, caption="C++ Solution"]
	int main(){
		return 0;	
	}
\end{lstlisting}

\end{solution}


%---Problem------------
\begin{problem}
\textit{Given an array of integers, sort the array according to frequency of elements. For example, if the input array is $\{2, 3, 2, 4, 5, 12, 2, 3, 3, 3, 12\}$, then modify the array to $\{3, 3, 3, 3, 2, 2, 2, 12, 12, 4, 5\}$}. 

If frequencies of two elements are same, print them in increasing order.

\textbf{Input:}
The first line of input contains an integer $T$ denoting the number of test cases. The description of $T$ test cases follows. The first line of each test case contains a single integer $N$ denoting the size of array. The second line contains $N$ space-separated integers $A1, A2, ..., AN$ denoting the elements of the array.

\textbf{Output:}
Print each sorted array on a newline. Each sorted array should be printed as space-separated.

\textbf{Constraints:}
\begin{multline}\\
1 \leq T \leq 70\\
1 \leq N \leq 130\\
1 \leq A[i] \leq 60\\
\end{multline}

\textbf{Example:}
\begin{verbatim}
Input:
1
5
5 5 4 6 4

Output:
4 4 5 5 6 

\end{verbatim}

\end{problem}

\begin{solution}
\begin{lstlisting}[language=C++, caption="C++ Solution"]
	int main(){
		return 0;	
	}
\end{lstlisting}

\end{solution}



%SKIENA EXERCICES!
\subsection{Skiens's Problems}

%---Problem------------
\begin{problem}
The Grinch is given the job of partitioning $2n$ players into two teams of $n$
players each. Each player has a numerical rating that measures how good he/she is
at the game. He seeks to divide the players as unfairly as possible, so as to create
the biggest possible talent imbalance between team A and team B. Show how the
Grinch can do the job in $O(n log n)$ time


\begin{solution}
Sort the array, so the first n elements are the the one with the best rating.
\hfill \\ \hfill \\
\begin{algorithm}[H]
 \KwData{Unsorted array A of 2n players}
 \KwResult{Two n-arrays of players (B,C)}
 $A \gets sort(A)$\;
 \For{$i\leftarrow 0$ \KwTo $n-1$}{
 	$B[i] \gets A[i]$\;
 	$C[i] \gets A[2n -1 - i]$\;
 }
\Return{(B,C)}\;

\caption{Priority Queue Extract-max pseudocode}
\end{algorithm}



\end{solution}
\end{problem}

%---Problem------------
\begin{problem}
\textit{The mode of a set of numbers is the number that occurs most frequently in the
set. The set $(4, 6, 2, 4, 3, 1)$ has a mode of $4$. Give an efficient and correct algorithm
to compute the mode of a set of $n$ number.}

\begin{solution}
Sort the array, so the equal element will be one right after the other. Start counting the occurrences of an element until the current element is different from the previous one. Now if the counter is greater than the current max, set max as the current counter.
\hfill \\ \hfill \\
\begin{algorithm}[H]
 \KwData{Unsorted array A of n elments}
 \KwResult{Element A[i], mode of A}
 $A \gets sort(A)$\;
 $nmax \gets 1 $\;
 $mode \gets A[i] $\;
 $prev \gets A[i] $\;
 $count \gets 1 $\;
 \For{$i\leftarrow 1$ \KwTo $n-1$}{
 	 \If{$number \neq A[i]$}{
 	 		  \If{$count > nmax$}{
					$nmax \gets count  $\;
					$mode \gets number  $\;
					
					
		 	 }
	 	 $number \gets A[i]  $\;
		$count \gets 0  $\;
 	 }
	$count \gets count +1 $\;
 }
\Return{mode}\;
\caption{}
\end{algorithm}


\end{solution}
\end{problem}

%---Problem------------
\begin{problem}
\textit{Given two sets $S_1$ and $S_2$ (each of size $n$), and a number $x$, describe an $O(n log n)$
algorithm for finding whether there exists a pair of elements, one from $S_1$ and one
from $S_2$ , that add up to $x$. (For partial credit, give a $\Theta(n^2 )$ algorithm for this
problem.)}

\begin{solution}
Naive quadratic solution work checking $\forall \;A[i] \; \exists \; A[j],\; i \neq j$ s.t. $A[i]+A[j] =x$, $j \in [i,n[$.
$O(nlog(n))$ solution is obtained by sorting the longest between $S_1$ and $S_2$. Then for each element of the shortest list find (using binary search) out if the element $x-S_{shortest}[i]$ is present in the longest. This will ensure a complexity of $O(nlog(n))$.
\hfill \\ \hfill \\
\begin{algorithm}[H]
 \KwData{Unsorted arrays $S_1$ and $S_2$}
 \KwResult{True iff a pair $(a\: \in S_1, b\: \in S_2)$ s.t. $a+b=x$ exists.}
 $S_1 \gets sort(S-1)$\;
 \For{$i\leftarrow 0$ \KwTo $sizeof(S_2)$}{
		 $idx \gets binarySearch(S_1,x-S_2[i])$\;
 	 \If{$isValid(idx)$}{
 	 		  \Return{$(S_1[i],S_2[idx])$}\;
 	 }
 }
\Return{NONE}\;
\caption{Priority Queue Extract-max pseudocode}
\end{algorithm}



\end{solution}
\end{problem}


%---Problem------------
\begin{problem}{Majority Element}
\textit{A majority element in an array $A$ of size $n$ is an element that appears more than $\frac{n}{2}$ times (and hence there is at most one such element). Find a solution that runs in $O(n)$}
Solution is the well known \textit{Boyers-Moore's majority vote algorithm}.
\begin{solution}
Basic idea around this algorithm is that majority element will appear more the sum of all other's element occurrences. In other word let $C_i$ be the  occurrences count 
for the majority element at position $i$. Let $C_{all} = \sum_{j=0,j\neq i}^n C_j$ then $ C_i - C_{all} > 0$.

It work in two distinc phases

\begin{enumerate}
\item Eliminate all element except one (the majority element candidate)
\item Check if the element is the majority element (simply count its occurrences and check that are more than $\frac{n}{2}$.

\end{enumerate}


\end{solution}
\end{problem}

