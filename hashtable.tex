Hash tables generalise the simple notion of direct addressing making effective uise of the ability to examine an arbtrary element of an array in $O(1)$ which we know is not affordable when the key universe is big, because it requires space which is proportional to the size of the universe. 
\subsection{Direct Addressing Table}
Suppose for instance that an application needs a synamic set in which each elemtn has a keu drqwn from the universe $U 0 \{0,1,\ldots m-1\} $ where $m$ is not too large and no towo element share the same key (indiex of the direct addressing table).
A direct addressing table is a indexable structure of size $m$ where each key of the universe has a one to one correspondence to one slot of the table. 
Dictionary operations can be performed as follows:

\begin{algorithm}
\Fn{SEARCH (T,k)}{
	$return T[k]$
 }
 \Fn{INSERT (T,x)}{
	$T[tokey(x)] = x$
 }
 \Fn{DELETE (T,x)}{
	$T[tokey(x)] = NIL$
 }
\caption{DIRECT ADDRESSING OPERATIONS}
\end{algorithm}

	\begin{figure}
	\label{fig:directaddressing}
	\centering
		\includegraphics{direct_addressing}
	\end{figure}

where $tokey$ is a function which takes an object $x$ and maps it to an integer in the interval $0\ldots m-1$. Such function is called $hashing function$. 
%---Problem------------
\begin{problem}
\textit{Suppose that a dynamic set S is represented by a direct-address table $T$ of length $m$. Describe a procedure that finds the maximum element of $S$. What is the worst-case performance of your procedure?}.

\begin{solution}
The direct addressing table is not sorted so binary search cannot be used. Only option is a linear scan.
\end{solution}
\end{problem}

Hash tables use addressing with indices as basic idea, but overcome the problem of using large space by using space which is proportional to the number of keys actually stored. The main idea is that we can compute an index from each key to be inserted, in $O(1)$ and use that index as the index of the array location where to store the object. Obviously a because the number of slots is less than the number of possible keys it is possible for two key to be stored at the same location hyelding to a \textit{collision}. There are various effective approach to handle collision which will be describer in the following sections.
In a hash table an element with is stored at slot $h(x)$ where $h$ is an hash function which maps the universe of keys $U$ into slots of the hash table.
\[
h(x): U \mapsto \{0,1,\ldots,m-1\}
\]
with $m$ typically much smaller than $|U|$. The hitch is that (pigeone priciple) two object may be assigned the same slot (they hash to the same slot). This situation is called a \textbf{collision}. There are several tecnquiques to deal with collisions such as 
\begin{enumerate}
\item chaining
\item open addressing
\item linear probing
\item quadratic probing
\item random probing
\end{enumerate}
Of course the best scenario is to avoid collision altogether but since the size of the universe if bigger than the size of the hash table there always exists at least a couple of object which will hash to the same slot. The hash function should, in order to minimize collision as much random as possible (remaining deterministic obviously). The word hash means to mix and it somehow evoke what this function does. It mixes data taken from the object in order to provide a random looking index to be used as an index of the hash table.

\subsection{Collision resolution by chaining}
In chaining, all the elements that hash to the same slot are placed into the same linked-list. The hash table can be viewed, infact, as an array of linked list where each node of the list contains apointer to the object stored.
Dictionary operation can so be performed in average case $O(1)$ as follows:
\begin{algorithm}
\Fn{SEARCH (T,x)}{
	$LIST-SEARCH(T[h(x)]$ 
	\tcc{Worst case - O(n) we will see that average case is much better O(1)}
 }
 \Fn{INSERT (T,x)}{
	$LIST-INSERT-HEAD(T[h(x)],x)$
	\tcc{O(1)}
 }
 \Fn{DELETE (T,x)}{
	$LIST-DELETE(T[h(x)],x)$
	\tcc{O(1) is list are doyhbly linked or O(n) if singly linked}
 }
\caption{Hash table - chaining dictionary operations }
\end{algorithm}

\subsubsection{ANALYSING PERFORMANCE OF HASHING WITH CHAINING}
Given an hash table $T$ we define the \textbf{load factor} ad the ration $\alpha=\frac{n}{m}$ where $n$ is the actual number of element stored into the hash table and $m$ is the dsize of the table.
The worst case scenario for hashtable with chaining is very bad. When all the inserted element hash to the same slot the seach procedure degrade to linear search in a list, with complexity $\Theta(n)$. We will show now that hash table performs much better on the average case and when certain operation such as enlarging or shrinking are performed carefully.
The main question here is: what is the complesxity for a hash table lookup operation? We evaulate the complexity of this operation with the length of the list being searched. A lookup operation may be succesful or unsuccesful. Analysing the former case is easy, since assuming that object are hashed with a \textbf{simple uniform hash function}\footnote{A function that hashes each object to one of the slots with uniform probability $Pr(h(obj)=j)=\frac{1}{m}$ and independently of the already hashed objects} while the latter requires a bit more work. Infact if we denote the length of the list $T[k]$  at location $k$ as $n_k$ then $n=\sum_{i=1}^{m-1} n_i$. The expected value for $n_j$ is then $\alpha$. 

In a unsuccesful lookup operation the list $T[j]$ must be traversed completly from head to tail giving so a complexity of $\Theta(1+\alpha)$ (computing of $j$ via the hasing function, plus the length of the list being searched). 
In a succesful lookup operation the list being search will be searched half way through on average case so the complexity will be $\approx\Theta(1+\frac{\alpha}{2})$.
Another approach would be to consider that supposing that the element being search lie in at location $j$ of list $T[l]$ all the element at indices $j-1,j-2,\ldots,0$ are inserted \textbf{after} the element being searched (because the insert operation push the element at the head of the list. In a succesful search then we examine all the element that have been inserted before. What is the probability that two element hash to the same slot? Again, $\frac{1}{m}$ because of the simple uniform hashing assumpion. So the cost of a single lookup is $1$ plus the number of element inserted after $j$ that hash to the same location of element $j$: 
\[
1+ \sum_{j=i+1}^n = \frac{1}{m}  = 1 + \frac{1}{m} \sum_{j=i+1}^n 1 =  1 + \frac{1}{m} (n-j)
\]
Taking the average over $n$ insertion we have 
\[
\frac{1}{n} \sum_{i=1}^n (1 + \frac{1}{m} (n-j)) = \frac{1}{n}(n + \frac{1}{m} \sum_{i=1}^n n -  \sum_{i=1}^n i)  = 1 +\frac{1}{nm} (n^2 -  \frac{n(n+1)}{2} ) = 1 + \frac{n^2}{nm} - (\frac{n^2}{2} + \frac{n}{nm}) = 1+ \alpha - \frac{\alpha}{2} + \frac{1}{2m}
\]
which is $\Theta(1+\alpha)$

What does this mean? That if we can keep  the value of $\alpha$ \textbf{constant} then we can perform all the dictionary operation in $O(1)$ average case! How can we keep $\alpha$ constant? If we keep $m$ is of the same order of $n$ then we have $\alpha = O(m)/n =(1)$ This tells us that the size of the hash table should be of the same order of the number of element stored in the has table itself. If we can ensure this invariant then dictionary operation can be performed in \textbf{constant time}!

\subsubsection{Alternative to Linked Lists}
Chaning with linked list in not obviouly the only possible solution. If we know that out universe is totally orderable than we could store the collisions in a binary search tree insted lowering the complexity from $\Theta(1+\alpha)$ to $\Theta(a+log(\alpha)$.


\subsection{Hash Functions}
A good hashing function satisfies the assumpion of simple uniform hashing. This condition is not easy to ensure a priori and is sometimes impossible to check because we rarly know the probability distribution of the key drwan. More importantly this keys may not be drawn independenlty. 
Heuristic are emplaced in the design of good hashing functions. For example in a compiler symbols table, it would be good for two similar words to hash in different slots (since for example two symbols like \texttt{pt} and \texttt{pts} may often occur in the same listing). 
Keys and object are often interpreted as natural numbers (and an important part of the hashing process is how we interpret the object as natural numbers. For instance a string may be interpreted as the sum of the ASCII values of all the character\footnote{Note that this is a terrible interpretation for a string since all the permutation of the same string willl produce the same key value!}). This key is then passed to another function which maps the keys into the most appropriate slot in the hash table. 

\subsubsection{Divison Method}
In the division method a key $k$ is mapped into one of the $m$ slots by taking the reminder of $k$ divided by $m$ (usually $ k> m$)
\[
h(m,k) = k \:mod\: m
\]
This methos is quite fast since it only requires a division. When using the divison method the size of the table, $m$ should not be a power of two since the division of $\frac{m=2^p}{k}$ consist of the p-lowest bits of $k$. For example let $m = 2^9 = 256_{10} = 010000000_2$ and $ k=300_{10}  = 100101010_2 \mapsto $ $k \: mod \:m = 000101010_2$ 
With this method the best choice is  prime number. But choosing $m$ prime does not solve all the problem or makes this method \textit{perfect} algtogether. All integer of the form $x+am$ will hash to the same slot $x$! Is quite easy then for a malicious agent to come up with a bad set of values that all hash to the same bucket.

\subsubsection{Multiplication Method}
The multiplication method operates in two steps: 
\begin{enumerate}
\item We multiply $k$ by a constant (called \textbf{salt}, chosen at creation time and remaining fixed for the entire lifetime of the hash table) $0<A<1$ and extract the fractional part of the result.
\item We then multiply this value by $m$ and take the floor of the result.
\end{enumerate}

\[
h(m,k) = \left \lfloor{m(Ak \: mod \: 1)}\right \rfloor 
\]
For instance suppose the key is $102$ and $A=\phi \approx 0.6180339887$ and $m = 30 $. The bucket is computed as follows:
\[
30 (\phi*102 - \left \lfloor \phi*102 \right \rfloor  ) =  \left \lfloor{30(63.0394668474 - 63 )}\right \rfloor = \left \lfloor{30*0.394668474}\right \rfloor  = \left \lfloor{11.84005422}\right \rfloor  = 11
\]
Since the fractional part of $Ak$ is a number always smaller than one, $h(m,k)$ is unsured to be in the interval $[0,m-1]$. A good advantage of this method is that the value of $m$ is not critical and infact we often choose $m$ to be a power of two since we can easily implement the method using shifts operators.


\begin{enumerate}
\item choose $m=2^p$
\item multiply $w$ bits of $k$ with $w$ bits of $(A *2^w)$ and we obtain a $2^w$ word number.
\item Extract the first $p$ bits of the lower hald of this product.
\end{enumerate}
A good choice as suggested by Knut is the  golden ration $\phi$.

Another good example in this class of hash function was analyzed by \textit{Dietzfelbinger et al.} in 1997, called binary multplicative hashing. The goal is to hash \textit{w}-bit integers (word size integer) to \textit{l}-bits (label) integers. 
we define such function as 
\[
d_a (x) = \left \lfloor{\frac{(ax) mod 2^w}{2^{w-l}}}\right \rfloor 
\]
with $a \in \{1,2,\ldots m-1\}$

\begin{lstlisting}[language=c++, caption="Multiplicative method hashing"]
typedef unsigned int uint;
#define INT_SIZE (32)
 int hash(const uint m,const  uint k, const uint a, const uint hash_size) {
 	return (a*k) >> (INT_SIZE-hash_size) 
 	//division by powers of 2 is equivalent to a right shift
 }

\end{lstlisting}
Note that the product $ak$ may be a $2^{2 \times INT\_SIZE}$ number, but this would cause overflow, so only the lower \texttt{INT\_SIZE} of the resulting product will be kept. Right shifting by the right amount will do the job of isolate the inner $l$ portion of the number. 
For instance consider the following listing:

\begin{lstlisting}[language=c++]
#include <iostream>
using namespace std;
typedef unsigned int uint;
int main() {
	uint a = 523456;
	uint res= a*a; //overflow
	cout<<(uint)res<<" "<<a<<endl;
	return 0;
}
\end{lstlisting}
\texttt{res} variable contains the number $3423244288$ which is the wrong answer since $523456^2=274006183936$. $3423244288$ correspond is a 32-bit integer which corresponds to the first 32-bits of the 64-bits number $274006183936$. $3423244288 = 274006183936 mod 2^{32}$.

\subsubsection{Universal Hashing}

Any possible scheme is vulnerable to the fact that is alway possible to choose $n$ objects whose keys maps to the same bucket, yelding as we know to the very bad $\Theta(n)$ lookup. The only approach that overcome this issue is for the hashing function to hash keys \textit{randomly} and \textit{indeentently}. In unversal hashing we select at the beginning of the the execution (at hash table  creation time if working in a object oriented programming environment) a suitable hash function from a set well designed class of functions. We will use randomizaion in the costruction of the hash function in ordder to ensure that for any sequence of element inserted in the table the expected performance will be good (i.e. the horrific $\Theta(n)$ lookup would occuper with a very little probability). This is the same kind of expectation quicksort assumes when choosing the pivot element.

\textit{To summarise when the keys to be inserted are uniformly drawn from a universe of keys is not difficould to come up with an hash function that distribute them uniformly across all the buckets meaning that the probability of collisions between any of two keys is the $\frac{1}{m}$ (any bucket at any time is equally likely to be chosen by the hash function). The main caveau is that the any assumption on the distribution of the keys to be inserted is not realistic manily because in real life keys are not drawn randomly, then because there could alway be the case then an adversary would attack our system submitting a patological sequence of keys which will hash to the same bucket. So a much more approach is to  make no asumption on the distribution of the keys and let randomess come into play in the definition of the hash function itself which is not now fixed. Looking now at the average running time of all the hash table operations withouth, again any assumpion on the distribution of the keys. An adversary cannot find a \textbf{bad sequence of keys} since the hash function is not known \textbf{a priori}. Randomess, which willl ensure good average case behaviour is in the definition of the hash function!} 

A family of function $\mathbb{H}$ is a  \textbf{universal hash function family} if each function the probability that two element collide is at most $\frac{1}{m}$ i.e. given two elements $x,y \in U$, $\forall h \in \mathbb{H}$ we have $Pr(h(x) = h(y)) \geq \frac{1}{m}$ (Sometimes this probability may be $O(\frac{1}{m})$ and the family is called $\epsilon$-almost universal). Not that universality does not implies uniformity\footnote{The propoerty such that all the possible hash values are equally likely to be returned. $Pr(h(x)=z) = \frac{1}{m}$}. Note also that uniformity is not terribly useful and is not a string enough condition to ensure good hash table performance on average. Take for instance the set $\mathbb{K} = \{ k_a(x) = a \: : 0\geq a<m\}$ of  constant function. This set is perfectly uniform in the sense that picking a random function $k_a$ at random each slot is equally likely to be choosen as destination slot. Probability that object $x$ hashes to slot $l$ is the probability that function $k_l$ is picked up at random from the set $\mathbb{K}$ which clearly is $\frac{1}{m}$. That is why minimizing the number of collision is much more important. The set $\mathbb{K}$ is uniform but is not unversal (according th the following definition) because the probability that picking up a random function $k_a$ and given two keys $x,y$ they hash to the same location is not less than $\frac{1}{m}$ but is $1$.


\begin{framed}
\textbf{Lineary of expectation} \hfill \\
By linearity of expectation we mean that the \textbf{expected value} of the sum of a random variable is equal to the sum of their individual expected values. It is essentially a weighted average of possible outcomes. Suppose we are interested in the expected value of the sum of $K$ dices. Calculating the the sum the usual way is tedious. We have to sum up all possible outcomes and divides by the total number of outcomes to get the expected values. 

\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{} & \textit{} &  & \textit{} & \textbf{SUM} \\ \hline
\textbf{(1,1)} & \textit{(1,1)} & \ldots & \textit{(1,6)} & $2+3+4+5+6+7=27 $ \\ \hline
\textbf{(2,1)} & \textit{(2,1)} & \ldots & \textit{(2,6)} & $3+4+5+6+7+8=33$ \\ \hline
\textbf{(3,1)} & \textit{(3,1)} & \ldots & \textit{(3,6)} & $4+5+6+7+8+9=39$ \\ \hline
\textbf{(4,1)} & \textit{(4,1)} & \ldots & \textit{(4,6)} & $5+6+7+8+9+10=45$ \\ \hline
\textbf{(5,1)} & \textit{(5,1)} & \ldots & \textit{(5,6)} & $6+7+8+9+10+11=51$ \\ \hline
\textbf{(6,1)} & \textit{(6,1)} & \ldots & \textit{(6,6)} & $7+8+9+10+11+12=57$ \\ \hline
\end{tabular}\hfill \\ \hfill \\

expected value is $\frac{57+51+45+39+33+27}{36}=\frac{252}{36}=7$. 
Linearity of expectation says that  the expected value of that sum of two dices is  the sum of the expected value of the single dices which is $\frac{1+2+3+4+5+6}{6}=3.5$. The expected value of the two dices is then $3.5\times 2$.
This also works if the dices have different expected values. Imagine a dice with number 3 substituted by number 99. The expected value of the sum of the two dices is then $3.5+19.5 = 23$.

Surprisingly this also works when the variables to be summed are \textbf{not independent}.

Suppose we want to find the expected value of the sum of two random variables $X$ and $Y$.
\[
E[X+Y] = \sum_x \sum_y (x+y) \times Pr(X=x,Y=y)  =
\]
then using associativity,
\[\sum_x \sum_y x \times Pr(X=x,Y=y) + \sum_x \sum_y y \times Pr(X=x,Y=y) \: \Longrightarrow\]

\[\sum_x x \sum_y  Pr(X=x,Y=y) + \sum_y y \sum_x   Pr(X=x,Y=y) \: \Longrightarrow \]

\[\sum_x x \sum_y   Pr(X=x) + \sum_y y \sum_x   Pr(Y=y)  \: \Longleftrightarrow \: E[x] + E[Y]\]
since $Pr(X=x,Y=y)= Pr(X=x)$ in the first part of the equation because summing up all the probabilities where X=x and Y can be whatever number gives the probability of $X=x$. Same logic applies to $Pr(X=x,Y=y)= Pr(Y=y)$ in the second part of the formula\footnote{What is the robability in a two dices drawning that $(X=1, Y=$whatever)? simply the probability that $X=1$.}.
Using induction on the number of variables this resuolt can be extende to an arbitrary number of variables. Continuous variables can be handles substituting summations with integrals.

\begin{example}
\textbf{There is a group on $n$ men all with a single hat on. The hats are redistribuited atrandom to every man. What is the expected number of hats each man get?}
This problems is easily solvable by means of linearity of expectation.  Let $N_i$ be the number of hats received by each man and $C_{ij}$ and indicator variables which is $1$ if and only of hat $i$ goes to man $j$.
\[
E[N_j] = E[ \sum_{i}^n C{ij}] = \sum_{i}^n E[C{ij}] = \sum_{i}^n Pr(C_{ij}=1)
\]
Since hats are uniformly distributed the probability that hat $i$ goes to man $j$ is $\frac{1}{n} \Rightarrow  \sum_{i}^n \frac{1}{n} = 1$ .

\end{example}

\begin{example}
\textbf{If the sum of two numbers rolled on the dice is $A$ and their product id $B$, compute E[A+B]}

We know that the expected value of the sum of two numbers rolled is $A=7$. In order to compute $B$ we apply linearity of expectation. What is the expected value of a single dice? $3.5$, so by linearity of expectation $3.5 \ times 3.5 = 12.25$. The soltuion is then: $E[A+B] = E[A]+E[B] = 7 +12.25$.
\end{example}


\begin{example}
\textbf{The digits $1,2,3,4$ are randomly arranged to form two two-digit numbers $AB$, and $CD$. What is the expected value of the product $AB \times CD$?}

We have to try to write the product as some kind of sum in order to apply the principle of linearity of expectation.
We should note that $AB \times CD = (10A+B) \times (10C+D) = 100AC + 10AD +10BC + BD$
This is an easier problem because by lineary of expecation we can sum up all the expected values for the product of the single digits $AC,AD,BC,BD$. What is then
\[
E[AC] =  E[AB] = E[BC]= E[BD] = \frac{1 \times 2 +  1 \times 3 + 1 \times 4 +2 \times 3 +2 \times 4 +4 \times 3 }{6} = \frac{35}{6}
\]
The final value is now easy to compute:
\[
100E[AC] + 10(E[AD] +E[BC]) + E[BD]) = 121 \times E[AC] = 705,8\overline{3} 
\]
This approach is much easier than computing the expected value listing explicitely$4!$ possible couple of two-digits numbers.
\end{example}


\begin{example}
\textbf{Let's take another example. Suppose you have a bag with 4 balls, each  of different color. You have to select 4 balls from the bag (with replacement). What is the expected number of different colors that will be extracted? }
in order to use the principle of linearity of expentancy, we should write the expected number of different colors as a simple sum of random variable for which is easy to compute their expected values.
Suppose now for a moment the following set of balls has been  extracted: \textbf{Red, Blue, Blue, Yellow}. If we set a indicator variable $P_c =$\textit{<color c has been extracted>} then $C_{green} + C_{blue} +C_{red} +C_{yellow} =   0 +1+1+1=3$. We have reduced the problem as a sum of simple to compute Indicator variable which expected value corresponds to the probability that the condition for which the indicator takes value one is true.
What is the probability that a color is selected? Is $1$ minus the probability is not selected.
More specifically, $Pr(C_c=1) = 1 - (\frac{3}{4})^4 = \frac{175}{256}$

Solution to the problem is then $E[C_{green}] + E[C_{blue}] +E[C_{red}] +E[C_{yellow}] = 4 \times \frac{175}{256}= 2,734375$

What if we have a bag with $n$ balls of different colors and we have to extract $n$ of them? How many different colors we are expected to extract?
Same principle : $Pr(C_c=1) = 1 - (\frac{n-1}{n})^4$
Solution would be $n \times (1-\frac{n-1}{n})^n$
\end{example}
\begin{example}
\textbf{25 independent, fair coins are tossed in a row. What is the expected number of consecutive HH pairs? If 6 coin tosses in a row give HHTHHH, the number of consecutive HH pairs is 3.}
There are 24 consecutive pairs: $HHH\ldots H$
Let use the indicator variable $C_i =1 $ if pair $i$ is $HH$. Probability that $C_i$ is $1$ is $\frac{1}{4}$.
$E[\sum_{i=1}^24 C_i] = \sum_{i=1}^24 E[C_i]$  by linearity of expectation.
\[
\sum_{i=1}^{24} \frac{1}{4} = \frac{24}{4} = 6
\]
So the expected number of consecutive $HH$ is 6.

Note that we could have solved the same problems noticing that:
if we denote p(n) the number of pairs in $n$ extraction then, p(1) = 0;
Successive extraction works as follows:

\begin{itemize}
	\item Extract H where previous coin was H (probability$ \frac{1}{4}$). This add one pair
	\item Extract H where previous coin was T Or extract T with whatever previous coin leave the number of consecutive head pair unchanged (probability$ \frac{3}{4}$, three cases HT,TH,TT). 
\end{itemize} 
We can write this as recurrence formula:
\[
	p(1) = 0;
	P(n) = \frac{3}{4}p(n) + \frac{1}{4} (p(n)+1) = p(n) + \frac{1}{4};
\]
Expanding $p(n)$ up to $n=25$ gives : $ \frac{1}{4} + \frac{1}{4} \ ldots + \frac{1}{4} $(24 times)
\end{example}

\begin{example}
\textbf{Coupon Collector: Suppose there are $n$ types of coupons in a lottery and each of them is equally likely to get extracted. What is the expected number of lots that have to be bought until we have at least a copy of \textit{all} coupons?}
Before to dive into the main problem is useful to stop and think to an easier version of this problem. Suppose the number of coupons is two. Let's think to the lottery as having three state:
\begin{enumerate}
	\item No coupons have been extracted.
	\item One coupon has been extracted (This state is always reched after $1$ extraction).
	\item Both coupons have been extracted. 
\end{enumerate}
State number $1$ only lasts for a trial while number $2$ is kept until we keep extraction the first coupon.
Let's denote by $X_i$ the number of extraction in state $i$. What we are really interested is $E[X_0]$. Obviously $E[X_2]=0$ since once we reach that state we don't need to extract any other coupon. What is the probability of a coupon to be extracted? $p=\frac{1}{2}$
$E[X_0] = 1 + E[X_1]$.  
$E[X_1] = 1 + (1-p)E[X_1] + pE[X_2] = 1+E[X_1]-pE[X_1]+0$
\[
E[X_1] =  1+E[X_1]-pE[X_1]+0 \Rightarrow 1 = pE[X_1]
\]
Hence $E[X_1] = \frac{1}{p} = 2$
This gives us an important insight.  Whenever we switch from state $i$ to $j$ and and the switching probability is $p$ the expected number of trials is $\frac{1}{p}$.
Going back to the original problem. The number of state is $n$ corrensponding to having $1,2,3 	\ldots, n$ different coupons. The idea is that if we know what is the expected number of trials for going from states $1 \rightarrow 2$ and then from states $2 \rightarrow 3$ and so on up to the last transition $n-1 \rightarrow n$ then $E[X_0] = 1 +  E[X_1] + E[X_2] +E[X_3] \ldots +E[X_{n-1}]$ What is then the probability of going from state $i$ to $j$? If we are in state $i$ we have collected $i$ coupons and we extract a \textit{new} one with probability $1 - \frac{i}{n}$ (Certainity minus the probability of extraction one of the already extracted coupons). We have already seen that given this probability $E[X_i] = \frac{1}{1 - \frac{i}{n}} = \frac{n}{n-i}$
\[
E[X_0] = 1+ \sum_{i=1}^{n-1} \frac{n}{n-i} = 1+ \Big(\frac{n}{n-1}+\frac{n}{n-2}+\frac{n}{n-3}+ \ldots \frac{n}{n-(n-1)}\Big)
\]
\[
= 1+n \Big(\frac{1}{n-1}+\frac{1}{n-2}+\frac{1}{n-3}+ \ldots 1 \Big) = 1 + nH_n
\]
Let's for instance fix $n=12$. The expected number of trials is then $1+12 \times (\frac{1}{1}+\frac{1}{2}+\frac{1}{3}+\ldots \frac{1}{11}) = 1+ 36.2358\overline{281385} =  37.2358\overline{281385}$ 
\end{example}


\end{framed}
%%linearity of expectation ------------------------------------------

Given a subset of $S \subseteq U$ of size $n$, $\forall x \in U$ the \textbf{expected} number of collisions between x and other element in S is at most $\alpha = \frac{n}{m}$. The total number of collision between $x$ and any other element from $S$ is $\sum_{y\neq x} Pr_{xy} < \frac{n}{m}$ by lineary of expectation. The expected value of the number of collision between $x$ and other elements of $S$ is the sum of the expected value for the collision between $x$ and the other elements.  This means that the expected number of elements in each slot is $\alpha$!

The proof to the above statement can be outlined as follows:
Let $C_x$ be a random variable indicating the total number of collision of keys in $S$ with $x$ and $C_{xy}$ be a random indicator variable  s.t. $C_{xy}=1 \Longleftrightarrow h(x)=h(y)$
The espected number of collision of $x$ is the expected sum of all the indicator variable $C_{xy}$.
\[
E[C_x] = E[\sum_{y\in S \setminus x} C_{xy}]
\]
By lineary of expectation, the expected value of a sum of a random variable is the sum of the expected values of each random variable so,
\[
E[C_x] = \sum_{y\in S \setminus x} E[C_{xy}]
\]
The expected value of an indicator random variable is its probability to be one. By definition of universal hashing we already know that $C_{xy}$ is one whenever a collision occurs, i.e. with probability $\frac{1}{m}$. 
\[
E[C_x] = \sum_{y\in S \setminus x} \frac{1}{m}
\]
There are $n-1$ element in $S \setminus x$,
\[
E[C_x] = \frac{n-1}{m} < \frac{n}{m}=\alpha
\]

\section{Direct Addressing}